# LyricGPT Tutorial

## Introduction

This tutorial will show you how to use [supabase](https://supabase.com/) and OpenAI's embbedings to search song lyrics using natural language.

After looking at different vector compatabile database, it was clear that supabase was the easiest to use.

This tutorial closely follows their tutorial on `Storing OpenAI embeddings in Postgres with pgvector`. (Source)[https://supabase.com/blog/openai-embeddings-postgres-vector]

## Create supabase project

After signing up for supabase, create your first project with default settings.

<div>
  <video width="800" height="auto" controls>
    <source
      src="https://videos.lyricgpt.io/createProjectSupa.mov"
      type="video/mp4"
    />
  </video>
</div>

Next we need to click on the terminal icon on the left sidebar. It should say SQL editor. We are going to pass in SQL commands to our supabase datbase.

I think the coolest thing about suopabase is that it is all based on postgres

The first command we are passing in is going to enable the vector extension:

```sql
create extension vector;
```

The next command creates the relevant tables for both the input to openai and the outputs.

```sql
create table documents (
  id bigserial primary key,
  content text,
  artist text,
  embedding vector (1536)
);
```

```sql
create or replace function match_documents_new (
  query_embedding vector(1536),
  similarity_threshold float,
  match_count int
)
returns table (
  id bigint,
  content text,
  artist text,
  title text,
  similarity float
)
language plpgsql
as $$
begin
  return query
  select
    documents.id,
    documents.content,
    documents.artist,
    documents.title,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where 1 - (documents.embedding <=> query_embedding) > similarity_threshold
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

Thinking about perfomance lets add an index:

```sql
create index on documents
using ivfflat (embedding vector_cosine_ops)
with (lists = 100);
```

Now in our vercel app we are going to write a function to generate the embeddings. This will be in the api folder.

```ts filename="app/api/generateEmbeddings/routes.ts"
import { createClient } from "@supabase/supabase-js";
import { Configuration, OpenAIApi } from "openai";

import * as fs from "fs";
import * as path from "path";
import { parse } from "csv-parse";

export async function GET() {
  const lyricPath = await getDocuments();
  let allSongRows: songRow[] = [];
  for (let i = 0; i < lyricPath.length; i++) {
    const path = lyricPath[i];
    const fileContent = fs.readFileSync(path, { encoding: "utf-8" });
    const parseCsv = parse(fileContent, {
      delimiter: ",",
      columns: headers,
    });
    await new Promise((resolve, reject) => {
      parseCsv.on("readable", () => {
        let record;
        while ((record = parseCsv.read())) {
          allSongRows.push(record);
        }
      });
      parseCsv.on("error", (err) => reject(err));
      parseCsv.on("end", () => resolve("done"));
    });
    for (let i = 0; i < allSongRows.length; i++) {
      const row = allSongRows[i];
      const songStringObj = `
      Artist: ${row.artist}
      Title: ${row.title}
      Lyrics: ${row.lyrics}`;
      const songString = songStringObj.replace(/\n/g, " ");
      console.log("right before embedding");
      const embeddingResponse = await openAi
        .createEmbedding({
          model: "text-embedding-ada-002",
          input: songString,
        })
        .catch((err) => {
          console.log("error", err);
        });
      if (!embeddingResponse) return;
      const [{ embedding }] = embeddingResponse.data.data;
      // In production we should handle possible errors
      await supabase.from("documents").insert({
        title: row.title,
        artist: row.artist,
        lyrics: row.lyrics,
        content: songString,
        embedding,
      });
    }

    // OpenAI recommends replacing newlines with spaces for best results
  }
  return new Response("Hello, Next.js!");
}

type songRow = {
  artist: string;
  id: string;
  lyrics_owner_id: string;
  primary_artist_id: string;
  primary_artist_name: string;
  song_art_image_thumbnail_url: string;
  title: string;
  url: string;
  pageviews: string;
  lyrics: string;
};

const headers = [
  "artist",
  "id",
  "lyrics_owner_id",
  "primary_artist_id",
  "primary_artist_name",
  "song_art_image_thumbnail_url",
  "title",
  "url",
  "pageviews",
  "lyrics",
];
async function getDocuments() {
  // get file paths of all csv
  const files = fs.readdirSync(folderPath); // Read the files synchronously
  return files.filter(isCsvFile).map(toFilePath); // Extract the CSV file paths
}

function isCsvFile(file: string) {
  return path.extname(file) === ".csv";
}

function toFilePath(file: string) {
  return path.join(folderPath, file);
}
```
